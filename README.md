# From Embeddings to Language Models: A Comparative Analysis of Feature Extractors for Text-Only and Multimodal Gesture Generation

### âš ï¸ Important Notice

ğŸ“… **The full implementation of the repository and pipeline configurations will be available after September 17th.**

**Johsac I. G. Sanchez, Paula D. P. Costa**
*ACM International Conference on Multimedia (ACM Multimedia) 2025*

ğŸ“„ **[Paper PDF](docs/paper.pdf)** | ğŸŒ **[Project Page/Videos]** 

---

![Pipeline Diagram](docs/pipelines.png)
*Figure 1: Flowchart of the evaluated experimental pipelines, from our paper.*

## ğŸ“ Abstract

Generating expressive and contextually appropriate co-speech gestures is crucial for naturalness in human-agent interaction. This study presents a systematic evaluation of seven gesture generation pipelines, comparing audio (WavLM, Whisper) and text (Word2Vec, Llama-3.2) feature extractors. We demonstrate that a smaller 3B-parameter LLM can achieve state-of-the-art performance, offering guidance for balancing generative quality with model accessibility.

## ğŸš€ Key Features

- ğŸ› ï¸ Implementation of 7 distinct gesture generation pipelines (multimodal and text-driven).
- ğŸ“¦ Pre-trained models for all evaluated pipelines, including \`Text-Only\`, \`Text-DiT\`, \`Multi-Dual\`, and more.
- ğŸ¥ Code to run inference and generate gestures from your own audio/text files.
- ğŸ“Š Scripts for objective evaluation using metrics like FGD, BAS, DS, APSD, JM, and Dice.
- ğŸ“¹ Supplementary videos showing qualitative results for all pipelines.

## ğŸ“‚ Project Structure

ğŸ“‚ LLM-Gesture-Pipelines  
â”£ ğŸ“œ README.md  
â”£ ğŸ“œ environment.yml  
â”£ ğŸ“œ Dockerfile  
â”£ ğŸ“œ .gitignore  
â”£ ğŸ“‚ DiffuseStyleGesture  
â”ƒ â”£ *(Content from the [DiffuseStyleGesture](https://github.com/YoungSeng/DiffuseStyleGesture.git) repository)*  
â”£ ğŸ“‚ data  
â”ƒ â”£ ğŸ“œ README.md  
â”ƒ â”£ ğŸ“‚ train  
â”ƒ â”ƒ â”£ ğŸ“„ sample_train.tsv  
â”ƒ â”£ ğŸ“‚ test  
â”ƒ â”ƒ â”£ ğŸ“„ sample_test.tsv  
â”£ ğŸ“‚ models  
â”ƒ â”£ ğŸ“‚ llm  
â”ƒ â”ƒ â”£ ğŸ“„ llama3b_config.py  
â”ƒ â”£ ğŸ“„ mdm.py  
â”ƒ â”£ ğŸ“„ mdm_1red_audtex.py  
â”ƒ â”£ ğŸ“„ mdm_1red_text.py  
â”ƒ â”£ ğŸ“„ mdm_2red_aud_tex.py  
â”ƒ â”£ ğŸ“„ mdm_llama3b.py  
â”ƒ â”£ ğŸ“„ mdm_dit_audtex.py  
â”ƒ â”£ ğŸ“„ mdm_dit_tex.py  
â”ƒ â”£ ğŸ“„ mdm_whisper_word2vec.py  
â”ƒ â”£ ğŸ“„ mdm_whisper_llama3b.py  
â”ƒ â”£ ğŸ“‚ pretrained  
â”ƒ â”ƒ â”£ ğŸ“œ README.md  
â”£ ğŸ“‚ scripts  
â”ƒ â”£ ğŸ“„ process_embedding_training.py  
â”ƒ â”£ ğŸ“„ train.py  
â”ƒ â”£ ğŸ“„ inference.py  
â”ƒ â”£ ğŸ“„ evaluate.py  
â”£ ğŸ“‚ evaluation  
â”ƒ â”£ ğŸ“œ environment.yml  
â”ƒ â”£ ğŸ“œ Dockerfile  
â”ƒ â”£ ğŸ“‚ metrics  
â”ƒ â”ƒ â”£ ğŸ“„ Metrics-results-generated_540k-llm.txt  
â”ƒ â”£ ğŸ“‚ videos  
â”ƒ â”ƒ â”£ ğŸ“œ README.md  
â”£ ğŸ“‚ examples  
â”ƒ â”£ ğŸ“„ generate_gestures.py  
â”ƒ â”£ ğŸ“„ sample_output.bvh  
â”ƒ â”£ ğŸ“‚ sample_input  
â”ƒ â”ƒ â”£ ğŸ“„ sample.wav  
â”ƒ â”ƒ â”£ ğŸ“„ sample.txt  
â”£ ğŸ“‚ docs  
â”ƒ â”£ ğŸ“„ paper.pdf  
â”ƒ â”£ ğŸ“„ setup_guide.md  
â”ƒ â”£ ğŸ“„ pipelines.png

## âš™ï¸ Setup & Installation

1. **Clone the repository**:
    ```bash
    git clone https://github.com/AI-Unicamp/LLM-Gesture-Pipelines.git
    cd LLM-Gesture-Pipelines
    ```

2. **Build the Docker image for training/inference**:
    ```bash
    docker build -t llm .
    ```

3. **Build the Docker image for evaluation**:
    ```bash
    cd evaluation
    docker build -t benchmarking_sdgg_models_image .
    cd ..
    ```

4. **Run the container for training/inference**:
    ```bash
    docker run --rm -it --gpus device=0 --userns=host --shm-size 64G -v /work/johsac.sanchez:/workspace/textdrive/ -p '8888:8888' --name my_container llm-gesture-pipeline:latest /bin/bash
    ```

5. **Run the container for evaluation** (if using separately):
    ```bash
    docker run --rm -it --gpus device=0 --name my_evaluation_container llm-gesture-evaluation:latest /bin/bash
    ```

6. **Install dependencies** (if not using Docker):
    ```bash
    conda env create -f environment.yml
    conda activate textgesture
    pip install pydub praat-parselmouth essentia TextGrid bvhsdk wandb
    conda install -c conda-forge ffmpeg h5py
    ```

## ğŸ› ï¸ Usage

### Preprocessing
Process audio (WavLM/Whisper) and text (Llama3.2/Word2Vec) embeddings:
```bash
conda run -n textgesture python scripts/process_embedding_training_v2_gg.py \
    --wavlm_path path/to/WavLM-Large.pt \
    --llm_model_path path/to/llama-3.2-3b-instruct \
    --wav_path data/wav/ \
    --txt_path data/tsv/ \
    --train_npy_path output/npy/
```

### Training
Train a model (e.g., model v6: DiT + WavLM + Llama3B):
```bash
conda run -n textgesture python scripts/train.py --model models/mdm_dit_audtex.py
```

### Inference
Generate gestures from audio and text inputs:
```bash
conda run -n textgesture python scripts/inference.py \
    --model models/mdm_dit_audtex.py \
    --audio examples/sample.wav \
    --text "Sample text" \
    --output examples/sample_output.bvh
```

### Evaluation
Compute objective metrics (FGD, BAS, DS, APSD, JM, Dice):
```bash
conda run -n textgesture python scripts/evaluate.py
```
Results available in `evaluation/metrics/Metrics-results-generated_540k-llm.txt`.

*Note*: If evaluation scripts are not compatible with PyTorch 2.1.0, use the `llm-gesture-evaluation` container.

## ğŸ“Š Metrics

Implemented metrics:
- **FGD**: FrÃ©chet Gesture Distance
- **BAS**: Beat Alignment Score
- **DS**: Diversity Score
- **JM**: Jerk Magnitude
- **Dice**: Dice Coefficient

Detailed results in `evaluation/metrics/`.

## ğŸ“¹ Videos

Evaluation videos available in `evaluation/videos/`. Follow instructions in `evaluation/videos/README.md` to download them.

## ğŸ“„ Citation

If you use this code or our models, please cite:
```bibtex
@inproceedings{sanchez2025embeddings,
  title={From Embeddings to Language Models: A Comparative Analysis of Feature Extractors for Text-Only and Multimodal Gesture Generation},
  author={Sanchez, Johsac Isbac Gomez and Costa, Paula Dornhofer Paro},
  booktitle={GENEA: Generation and Evaluation of Non-verbal Behaviour for Embodied Agents Workshop 2025},
  year={2025}
}
```

## ğŸ“š Acknowledgments

- Based on [DiffuseStyleGesture](https://github.com/YoungSeng/DiffuseStyleGesture.git).
- Thanks to the libraries: PyTorch, Transformers, Librosa, and others.

## ğŸ“§ Contact

For questions or contributions, contact [paulad@unicamp.br](mailto:paulad@unicamp.br).
